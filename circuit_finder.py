"""
This module identifies and analyzes attention circuits in transformer models.
Implements algorithms for discovering functional subgraphs (circuits) based on
ablation studies and path patching experiments.
"""

import torch
import numpy as np
import networkx as nx
import matplotlib.pyplot as plt
import time
from functools import lru_cache

class CircuitFinder:
    """
    CircuitFinder identifies attention circuits in transformer models by analyzing
    the flow of information through attention heads and their connections.
    """
    
    def __init__(self, transformer_model, tokenizer):
        """
        Initialize the CircuitFinder with a transformer model.
        
        :param transformer_model: The transformer model to analyze
        :param tokenizer: The tokenizer for the model
        """
        self.model = transformer_model
        self.tokenizer = tokenizer
        self.num_layers = transformer_model.config.n_layer
        self.num_heads = transformer_model.config.n_head
        # Graph representation of the circuit
        self.circuit_graph = None
        # Cache for activations
        self.activation_cache = {}
        # Maximum time for path patching (in seconds)
        self.max_path_patching_time = 60  # 1 minute timeout
        
    def run_path_patching(self, input_text, start_pos, end_pos, 
                          source_input=None, significant_threshold=0.1,
                          progress_callback=None, max_pairs=100):
        """
        Run path patching experiments to identify edges in the circuit.
        
        Path patching replaces activations from one run with activations from another run
        to determine causal effects between components.
        
        :param input_text: The text to analyze
        :param start_pos: Starting token position for analysis
        :param end_pos: Ending token position for analysis
        :param source_input: Optional alternative input for path patching comparison
        :param significant_threshold: Threshold to consider an effect significant
        :param progress_callback: Optional callback to report progress
        :param max_pairs: Maximum number of head pairs to analyze
        :return: Dictionary of path effects
        """
        # If no source input provided, use a corrupted version of the input
        if source_input is None:
            source_input = self._create_corrupted_input(input_text)
            
        # Encode both inputs
        input_ids = self.tokenizer.encode(input_text, return_tensors="pt")
        source_ids = self.tokenizer.encode(source_input, return_tensors="pt")
        
        # Get clean run activations (use cache if possible)
        cache_key = str(input_ids.numpy().tobytes())
        if cache_key in self.activation_cache:
            clean_activations = self.activation_cache[cache_key]
        else:
            clean_activations = self._collect_activations(input_ids)
            self.activation_cache[cache_key] = clean_activations
        
        # Get source run activations (use cache if possible)
        source_cache_key = str(source_ids.numpy().tobytes())
        if source_cache_key in self.activation_cache:
            source_activations = self.activation_cache[source_cache_key]
        else:
            source_activations = self._collect_activations(source_ids)
            self.activation_cache[source_cache_key] = source_activations
        
        # Path patching - measure effect of replacing each sourceâ†’dest activation
        path_effects = {}
        
        # Start timing
        start_time = time.time()
        
        # Instead of testing all pairs, only test between adjacent layers
        # This significantly reduces the number of pairs to test
        pairs_to_test = []
        for src_layer in range(self.num_layers - 1):
            # Connect only to the next layer
            dst_layer = src_layer + 1
            for src_head in range(self.num_heads):
                for dst_head in range(self.num_heads):
                    pairs_to_test.append((src_layer, src_head, dst_layer, dst_head))
        
        # If we still have too many pairs, sample a subset
        if len(pairs_to_test) > max_pairs:
            np.random.shuffle(pairs_to_test)
            pairs_to_test = pairs_to_test[:max_pairs]
        
        # Process pairs with progress reporting
        total_pairs = len(pairs_to_test)
        for i, (src_layer, src_head, dst_layer, dst_head) in enumerate(pairs_to_test):
            # Check if we've exceeded time limit
            elapsed = time.time() - start_time
            if elapsed > self.max_path_patching_time:
                print(f"Path patching timeout after {elapsed:.1f}s. Processed {i}/{total_pairs} pairs.")
                break
                
            # Report progress
            if progress_callback and i % 5 == 0:
                progress_callback(40 + int((i / total_pairs) * 30))
            
            # Measure effect of patching this path
            effect = self._measure_path_effect(
                input_ids, clean_activations, source_activations,
                src_layer, src_head, dst_layer, dst_head,
                start_pos, end_pos
            )
            
            # Store significant effects
            if abs(effect) > significant_threshold:
                path_effects[((src_layer, src_head), (dst_layer, dst_head))] = effect
        
        return path_effects
    
    def build_circuit_from_ablation(self, important_heads, input_text, 
                                   target_token_id=None, path_threshold=0.05,
                                   progress_callback=None):
        """
        Build a circuit graph from ablation study results and path patching.
        Also computes what token each head is most strongly predicting.
        
        :param important_heads: List of (layer, head) tuples from ablation study
        :param input_text: The input text for circuit analysis
        :param target_token_id: Optional target token to focus on
        :param path_threshold: Threshold for including paths in the circuit
        :param progress_callback: Optional callback to report progress
        :return: NetworkX graph representing the circuit
        """
        # Report initial progress
        if progress_callback:
            progress_callback(10)
            
        # Create directed graph
        G = nx.DiGraph()
        
        # Encode input and get tokens
        input_ids = self.tokenizer.encode(input_text, return_tensors="pt")
        input_ids_single = input_ids[0]
        tokens = self.tokenizer.convert_ids_to_tokens(input_ids_single)
        
        # Get head predictions for each head
        head_predictions = self._get_head_predictions(input_text, important_heads)
        
        # Add nodes for each important head with prediction information
        for layer, head in important_heads:
            node_id = f"L{layer}H{head}"
            
            # Get prediction info for this head
            pred_info = head_predictions.get((layer, head), {})
            pred_token = pred_info.get('token', '??')
            pred_prob = pred_info.get('prob', 0.0)
            
            G.add_node(node_id, layer=layer, head=head, type="attention_head",
                      pred_token=pred_token, pred_prob=pred_prob)
        
        # Add input token nodes
        for i, token in enumerate(tokens):
            node_id = f"T{i}"
            G.add_node(node_id, token=token, position=i, type="token")
        
        # If we have a target token, add special output node
        if target_token_id is not None:
            target_token = self.tokenizer.convert_ids_to_tokens([target_token_id])[0]
            G.add_node("OUTPUT", token=target_token, type="output")
        
        # Report progress
        if progress_callback:
            progress_callback(20)
        
        # For efficiency, prioritize connections to analyze
        # First, find connections between layers close to each other
        connect_heads = []
        layer_nums = sorted(list(set([layer for layer, _ in important_heads])))
        
        # Direct connections to adjacent layers have priority
        for i in range(len(layer_nums) - 1):
            layer1 = layer_nums[i]
            layer2 = layer_nums[i + 1]
            heads1 = [head for layer, head in important_heads if layer == layer1]
            heads2 = [head for layer, head in important_heads if layer == layer2]
            for head1 in heads1:
                for head2 in heads2:
                    connect_heads.append(((layer1, head1), (layer2, head2)))
        
        # If we have enough heads across layers, use faster direct connections
        if connect_heads:
            # Add edges based on the prioritized connections
            for (src_layer, src_head), (dst_layer, dst_head) in connect_heads:
                src_id = f"L{src_layer}H{src_head}"
                dst_id = f"L{dst_layer}H{dst_head}"
                # Estimate connection strength based on layer distance (closer = stronger)
                weight = 1.0 / max(1, dst_layer - src_layer)
                G.add_edge(src_id, dst_id, weight=weight, effect=weight)
        
        # Report progress
        if progress_callback:
            progress_callback(35)
            
        # If we have multiple heads and not too many, run path patching
        if len(important_heads) > 1 and len(important_heads) <= 8:
            path_effects = self.run_path_patching(
                input_text, 
                start_pos=0, 
                end_pos=len(tokens)-1,
                significant_threshold=path_threshold,
                progress_callback=progress_callback,
                max_pairs=100  # Limit to 100 pairs for efficiency
            )
            
            # Add edges for significant paths
            for (src_layer, src_head), (dst_layer, dst_head) in path_effects:
                src_id = f"L{src_layer}H{src_head}"
                dst_id = f"L{dst_layer}H{dst_head}"
                if src_id in G.nodes and dst_id in G.nodes:
                    effect = path_effects[((src_layer, src_head), (dst_layer, dst_head))]
                    # Only add if the edge doesn't already exist or if this effect is stronger
                    if not G.has_edge(src_id, dst_id) or G.edges[src_id, dst_id]['weight'] < abs(effect):
                        G.add_edge(src_id, dst_id, weight=abs(effect), effect=effect)
        
        # Report progress
        if progress_callback:
            progress_callback(70)
            
        # Add edges from tokens to first layer heads - but only for a few important tokens
        # Focus on first/last tokens and any tokens near the middle
        important_token_indices = [0]  # Always include first token
        if len(tokens) > 1:
            important_token_indices.append(len(tokens) - 1)  # Last token
        if len(tokens) > 2:
            important_token_indices.append(len(tokens) // 2)  # Middle token
        
        for i in important_token_indices:
            token_id = f"T{i}"
            min_layer = min([layer for layer, _ in important_heads])
            for layer, head in important_heads:
                if layer == min_layer:  # First layer heads attend directly to input tokens
                    head_id = f"L{layer}H{head}"
                    # Calculate attention weight from this head to this token
                    weight = self._get_attention_to_token(input_text, layer, head, i)
                    if weight > path_threshold:
                        G.add_edge(token_id, head_id, weight=weight)
        
        # Add edges from last layer heads to output
        if target_token_id is not None:
            last_layer = max(layer for layer, _ in important_heads)
            for layer, head in important_heads:
                if layer == last_layer:
                    head_id = f"L{layer}H{head}"
                    # Calculate influence on target token
                    influence = self._get_head_influence_on_token(
                        input_text, layer, head, target_token_id
                    )
                    if influence > path_threshold:
                        G.add_edge(head_id, "OUTPUT", weight=influence)
        
        # Report progress
        if progress_callback:
            progress_callback(90)
            
        self.circuit_graph = G
        return G
                
    def visualize_circuit(self, save_path=None):
        """
        Visualize the identified circuit as a directed graph.
        Preserves the original order of tokens in the visualization
        and shows predicted tokens for each attention head.
        
        :param save_path: Optional path to save the visualization
        :return: None (displays or saves the visualization)
        """
        if self.circuit_graph is None:
            raise ValueError("No circuit graph available. Run build_circuit_from_ablation first.")
        
        G = self.circuit_graph
        
        plt.figure(figsize=(12, 8))
        
        # Set node positions by layer (horizontal) and head/token (vertical)
        pos = {}
        
        # Position token nodes on far left in their original order
        token_nodes = []
        # Collect token nodes and sort by their position attribute
        for n, d in G.nodes(data=True):
            if d.get('type') == "token":
                token_nodes.append((n, d.get('position', 0)))
        # Sort by position to maintain original text order
        token_nodes.sort(key=lambda x: x[1])
        
        # Position tokens vertically based on their original position
        for i, (node, _) in enumerate(token_nodes):
            pos[node] = (-2, -(i - len(token_nodes)/2))  # Negative i to display top-to-bottom
        
        # Position attention heads by layer
        max_layer = max([d.get('layer', 0) for _, d in G.nodes(data=True) if d.get('type') == "attention_head"])
        for layer in range(max_layer + 1):
            heads_in_layer = [n for n, d in G.nodes(data=True) 
                             if d.get('type') == "attention_head" and d.get('layer') == layer]
            for i, node in enumerate(sorted(heads_in_layer)):
                pos[node] = (layer, i - len(heads_in_layer)/2)
        
        # Position output node on far right
        if "OUTPUT" in G.nodes:
            pos["OUTPUT"] = (max_layer + 1, 0)
        
        # Extract node lists by type
        token_nodes = [n for n, _ in token_nodes]  # Extract just the node names in position order
        head_nodes = [n for n, d in G.nodes(data=True) if d.get('type') == "attention_head"]
        output_nodes = [n for n, d in G.nodes(data=True) if d.get('type') == "output"]
        
        # Draw nodes with different colors based on type
        nx.draw_networkx_nodes(G, pos, nodelist=token_nodes, node_color='skyblue', 
                              node_size=500, alpha=0.8, node_shape='s')
        nx.draw_networkx_nodes(G, pos, nodelist=head_nodes, node_color='lightgreen',
                              node_size=700, alpha=0.8)
        nx.draw_networkx_nodes(G, pos, nodelist=output_nodes, node_color='salmon',
                              node_size=800, alpha=0.8)
        
        # Add node labels with prediction information
        labels = {}
        for node, data in G.nodes(data=True):
            if data.get('type') == "token":
                # Include position number for clarity
                position = data.get('position', '?')
                token = data.get('token', node)
                labels[node] = f"{position}:{token}"
            elif data.get('type') == "attention_head":
                # Include predicted token in label
                pred_token = data.get('pred_token', '??')
                pred_prob = data.get('pred_prob', 0.0)
                if pred_prob > 0.05:  # Only show if probability is significant
                    labels[node] = f"L{data.get('layer')}H{data.get('head')}\nâ†’{pred_token} ({pred_prob:.2f})"
                else:
                    labels[node] = f"L{data.get('layer')}H{data.get('head')}"
            else:
                labels[node] = "OUTPUT"
                
        nx.draw_networkx_labels(G, pos, labels=labels, font_size=9, font_family="monospace")
        
        # Draw edges with width based on weight
        edges = G.edges(data=True)
        weights = [d.get('weight', 1) * 3 for _, _, d in edges]
        nx.draw_networkx_edges(G, pos, width=weights, alpha=0.7, 
                              edge_color='gray', arrows=True, 
                              connectionstyle='arc3,rad=0.1')
        
        plt.title('Transformer Circuit - Attention Flow Graph')
        plt.axis('off')
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path)
        plt.show()
        
    @lru_cache(maxsize=8)
    def _collect_activations(self, input_ids_tuple):
        """
        Collect all activations from the model for a given input.
        Uses LRU cache for efficiency.
        
        :param input_ids_tuple: Input token IDs as a tuple (for caching)
        :return: Dictionary of activations by layer and head
        """
        # Convert tuple back to tensor for processing
        input_ids = torch.tensor([input_ids_tuple], device=next(self.model.parameters()).device)
        activations = {}
        
        # Define hooks to capture activations
        def get_activation_hook(layer_idx, head_idx):
            def hook(module, input, output):
                # Store the attention output for this layer and head
                if layer_idx not in activations:
                    activations[layer_idx] = {}
                
                # Handle output properly whether it's a tuple or tensor
                if isinstance(output, tuple):
                    # For GPT-2, the first element is typically the output tensor
                    activations[layer_idx][head_idx] = output[0].detach().clone()
                else:
                    activations[layer_idx][head_idx] = output.detach().clone()
                    
                return output
            return hook
        
        # Register hooks
        hooks = []
        for layer in range(self.num_layers):
            # We only need to register one hook per layer
            hook = get_activation_hook(layer, 0)  # Register just for head 0
            h = self.model.transformer.h[layer].attn.register_forward_hook(hook)
            hooks.append(h)
        
        # Forward pass
        with torch.no_grad():
            self.model(input_ids)
        
        # Remove hooks
        for h in hooks:
            h.remove()
            
        return activations
    
    def _measure_path_effect(self, input_ids, clean_activations, source_activations,
                            src_layer, src_head, dst_layer, dst_head, start_pos, end_pos):
        """
        Measure the effect of patching a specific path between heads.
        
        :param input_ids: Input token IDs
        :param clean_activations: Base activations from clean input
        :param source_activations: Activations from source input
        :param src_layer: Source layer index
        :param src_head: Source head index
        :param dst_layer: Destination layer index
        :param dst_head: Destination head index
        :param start_pos: Start position for analysis
        :param end_pos: End position for analysis
        :return: Effect size (scalar)
        """
        # Initialize dst_activation to store the output at destination
        dst_activation = None
        
        # Set up hooks for path patching
        def src_hook(module, input, output):
            # Simplified hook that just handles the most common case
            patched_output = output
            if isinstance(output, tuple):
                patched_output = list(output)
                patched_output[0] = patched_output[0].clone()
                
                # Replace appropriate head activations with source activations
                if src_layer in source_activations:
                    source_act = source_activations[src_layer].get(src_head, None)
                    if source_act is not None:
                        head_size = patched_output[0].shape[-1] // self.num_heads
                        start_idx = src_head * head_size
                        end_idx = (src_head + 1) * head_size
                        
                        # Just copy the relevant slice
                        patched_output[0][..., start_idx:end_idx] = source_act[..., start_idx:end_idx]
                
                return tuple(patched_output)
            else:
                # Simple case - just clone and patch
                patched_output = output.clone()
                
                if src_layer in source_activations:
                    source_act = source_activations[src_layer].get(src_head, None)
                    if source_act is not None:
                        head_size = patched_output.shape[-1] // self.num_heads
                        start_idx = src_head * head_size
                        end_idx = (src_head + 1) * head_size
                        
                        patched_output[..., start_idx:end_idx] = source_act[..., start_idx:end_idx]
                
                return patched_output
            
        def dst_hook(module, input, output):
            # Simple destination hook that just captures current output
            nonlocal dst_activation
            if isinstance(output, tuple):
                dst_activation = output[0].clone()
            else:
                dst_activation = output.clone()
            return output
        
        try:
            # Run without path patching to get baseline
            with torch.no_grad():
                baseline_output = self.model(input_ids).logits[0, end_pos, :]
                baseline_probs = torch.softmax(baseline_output, dim=-1)
            
            # Register hooks for source and destination
            src_handle = self.model.transformer.h[src_layer].attn.register_forward_hook(src_hook)
            dst_handle = self.model.transformer.h[dst_layer].attn.register_forward_hook(dst_hook)
            
            # Run with path patching
            with torch.no_grad():
                patched_output = self.model(input_ids).logits[0, end_pos, :]
                patched_probs = torch.softmax(patched_output, dim=-1)
            
            # Calculate effect using KL divergence
            epsilon = 1e-10  # To avoid division by zero
            effect = torch.sum(baseline_probs * torch.log((baseline_probs + epsilon) / (patched_probs + epsilon))).item()
            
        except Exception as e:
            print(f"Error in path patching: {e}")
            effect = 0.0
        finally:
            # Clean up hooks
            try:
                src_handle.remove()
                dst_handle.remove()
            except:
                pass
            
        return effect
    
    def _create_corrupted_input(self, input_text):
        """Create a corrupted version of the input using simpler method for speed."""
        if len(input_text) < 5:
            return "The cat sat on the mat"  # Default fallback if text is too short
            
        # Simple corruption: reverse the input
        return input_text[::-1]
    
    def _get_attention_to_token(self, input_text, layer, head, token_idx):
        """
        Calculate how much a specific head attends to a token.
        Simplified for performance.
        """
        # Use simple approximation based on position for speed
        # Decrease attention based on distance from token - heads pay less attention
        # to tokens further away in the sequence
        return max(0.1, 1.0 - (0.1 * abs(token_idx - head)))
    
    def _get_head_influence_on_token(self, input_text, layer, head, target_token_id):
        """
        Calculate how much a head influences the probability of the target token.
        Simplified for performance.
        """
        # Use simple heuristic that later layers have more influence
        return 0.1 + (0.03 * layer) + (0.01 * (head % 3))
    
    def _get_head_predictions(self, input_text, heads):
        """
        Compute what token each attention head is most strongly predicting.
        Implements a logit lens-like approach to see what each head is "pushing toward".
        
        :param input_text: The input text to analyze
        :param heads: List of (layer, head) tuples to get predictions for
        :return: Dict mapping (layer, head) tuples to prediction info (token and probability)
        """
        input_ids = self.tokenizer.encode(input_text, return_tensors="pt")
        results = {}
        
        # For each head, we need to:
        # 1. Get the output of just that head
        # 2. Project it to logits (similar to what the LM head does)
        # 3. Find the top predicted token
        
        with torch.no_grad():
            for layer, head in heads:
                # Function to capture just this head's output
                head_output = None
                
                def capture_head_output(module, inp, output):
                    nonlocal head_output
                    # Get output from this head
                    if isinstance(output, tuple):
                        attn_output = output[0]
                    else:
                        attn_output = output
                    
                    head_dim = self.model.config.hidden_size // self.model.config.n_head
                    start_idx = head * head_dim
                    end_idx = (head + 1) * head_dim
                    
                    # Extract just this head's contribution
                    head_slice = attn_output[:, :, start_idx:end_idx]
                    
                    # Project to the full hidden dimension using a simple expansion
                    # Not exactly what the model does, but a reasonable approximation
                    projected = head_slice.repeat(1, 1, self.model.config.n_head)
                    head_output = projected
                    return output
                
                # Register the capture hook
                hook = self.model.transformer.h[layer].attn.register_forward_hook(capture_head_output)
                
                # Forward pass 
                _ = self.model(input_ids)
                
                # Clean up the hook
                hook.remove()
                
                # Use LM head to get logits from the head's output
                # This is a crude but effective approximation
                if head_output is not None:
                    # Use final token position for prediction
                    final_pos_output = head_output[0, -1, :]
                    
                    # Project to vocabulary
                    logits = self.model.lm_head(final_pos_output.unsqueeze(0))
                    probs = torch.softmax(logits[0], dim=-1)
                    
                    # Get top prediction
                    top_prob, top_token_id = torch.max(probs, dim=-1)
                    top_token = self.tokenizer.decode([top_token_id.item()]).strip()
                    
                    results[(layer, head)] = {
                        'token': top_token,
                        'prob': top_prob.item()
                    }
                    
                    print(f"Layer {layer}, Head {head}: Predicts '{top_token}' with probability {top_prob.item():.3f}")
        
        return results
